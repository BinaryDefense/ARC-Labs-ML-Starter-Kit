{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Synthetic Data using Llama 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session, we will explore how to use Llama 3.1 to generate synthetic logs for testing and analyzing systems, particularly focusing on Windows Event Log 7045, both benign (normal) and malicious (suspicious) logs. \n",
    "\n",
    "Here is what we will cover in this session:\n",
    "\n",
    "- Use Llama 3.1 to generate synthetic logs.\n",
    "- Define system and user content to shape model behavior.\n",
    "- Create both benign and malicious logs for simulating real-world data.\n",
    "- Parse unstructured log text into a structured format for easier processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirement:\n",
    "    https://aimlapi.com/\n",
    "    api_key is provided: ed4b5e9d497f4d8badf2ed3929bb0c2d\n",
    "\n",
    "    !pip install openai\n",
    "    !pip install pandas\n",
    "\n",
    "Optional:\n",
    "    All package installations can be done in a requirement file.\n",
    "    Add a requirement.txt file in the same dir\n",
    "    Run the following command:\n",
    "    !pip install -r requirements.txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Using cached openai-1.43.1-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Using cached anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\python312\\lib\\site-packages (from openai) (1.9.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\python312\\lib\\site-packages (from openai) (0.5.0)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Using cached pydantic-2.9.0-py3-none-any.whl.metadata (146 kB)\n",
      "Requirement already satisfied: sniffio in c:\\python312\\lib\\site-packages (from openai) (1.3.1)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\python312\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\python312\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached pydantic_core-2.23.2-cp312-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tzdata in c:\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2024.1)\n",
      "Collecting colorama (from tqdm>4->openai)\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Using cached openai-1.43.1-py3-none-any.whl (365 kB)\n",
      "Using cached anyio-4.4.0-py3-none-any.whl (86 kB)\n",
      "Using cached httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Using cached pydantic-2.9.0-py3-none-any.whl (434 kB)\n",
      "Using cached pydantic_core-2.23.2-cp312-none-win_amd64.whl (1.9 MB)\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Installing collected packages: pydantic-core, colorama, certifi, anyio, annotated-types, tqdm, pydantic, httpcore, httpx, openai\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.4.0 certifi-2024.8.30 colorama-0.4.6 httpcore-1.0.5 httpx-0.27.2 openai-1.43.1 pydantic-2.9.0 pydantic-core-2.23.2 tqdm-4.66.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.2-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\python312\\lib\\site-packages (from pandas) (2.1.1)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pandas-2.2.2-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Installing collected packages: python-dateutil, pandas\n",
      "Successfully installed pandas-2.2.2 python-dateutil-2.9.0.post0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\John\\\\Desktop', 'C:\\\\Users\\\\John\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python311.zip', 'C:\\\\Users\\\\John\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib', 'C:\\\\Users\\\\John\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\DLLs', 'C:\\\\Users\\\\John\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311', '', 'C:\\\\Users\\\\John\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages', 'C:\\\\Users\\\\John\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\John\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\John\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages\\\\Pythonwin']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print (sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'openai'"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The system content serves as the backbone of any structured conversation in AI. Think of it as the part of the model where we set the tone, establish boundaries, and provide essential context for guiding responses. This system role ensures that the model adheres to specific rules, whether it's staying on-topic, maintaining a certain behavior, or even following safety protocols. It’s like giving the model a playbook that it uses to shape its replies in a way that aligns with your goals. \n",
    "\n",
    "    we define the system content:\n",
    "        you are <an expert in cybersecurity>.  you task is <to generate synthetic Windows Event ID 7045 log entries for training purposes>.\n",
    "        <each entry> should include:\n",
    "        - a\n",
    "        - b\n",
    "        - c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system content \n",
    "\n",
    "system_content = \"\"\"\n",
    "You are an expert in cybersecurity. Your task is to generate synthetic Windows Event ID 7045 log entries for training purposes.\n",
    "Each entry should include:\n",
    "- A label (\"benign\" or \"malicious\")\n",
    "- Service Name\n",
    "- Service File Name\n",
    "- Service Type\n",
    "- Service Start Type\n",
    "- Service Account\n",
    "- Data Service Name\n",
    "- Timestamp\n",
    "- ID\n",
    "\n",
    "For the ID:\n",
    "- End with \"x\" for malicious entries\n",
    "- End with \"Y\" for benign entries\n",
    "For the Service File Name:\n",
    "- Use a command line for the malicious entries\n",
    "- Use a file path for the benign entries\n",
    "Ensure the generated entries are varied and realistic.\n",
    "\n",
    "An example of a benign entry might look like this:\n",
    "- Label: benign\n",
    "- Service Name: sysmon\n",
    "- Service File Name: C:\\WINDOWS\\sysmon.exe\n",
    "- Service Type: auto start\n",
    "- Service Start Type: user mode service\n",
    "- Account Name: LocalSystem\n",
    "- Data Service Name: Windows11\n",
    "- Timestamp: 2024-08-04T17:58:19Z\n",
    "- ID: fc7deb2c-9f43-49de-aff0-xxxxxxxxxxxxB\n",
    "\n",
    "An example of a malicious entry might look like this:\n",
    "- Label: malicious\n",
    "- Service Name: MTsMjDat\n",
    "- Service File Name: \"powershell -nop -w hidden -noni -c '=New-Object IO.MemoryStream(,[Convert]::FromBase64String(TgBrAHIAVABoAEEAWAAzAHYAbwBiAHMAUAAwAGEATgBXAHAAMwA5AHQAWABQADgAcABiAGUARgBBAG8AVgAwAHkAYgBPADkAUgBnAGIAMABKAFMANgByAGoATwB5ADAAZwBwAHYAaQAyADgANQBCAGEAdgA3AE4ARABMAGEAagBaAHMAQQBmADYAcABVAGEASwBzAEQAagAwADgARABFAFoATABvAEYAUgBiAGYARQBuADUAcABkAEkANAB2AFcAMQBRAFgAUABqAFEANQBlAGIAUQBmAFQAZwBjAFMAYwBqADMAawBxAFEAZwBmADIAYQBuAG4AWABjADEANgAyAFAANABKAEoAZwBUAEMANgBvAHYAWgBIAFAARQB4AEcAYwBYAEEAbAB4AEMAZwBOAE8AWQBPAFcAaQBQAEQAMQBWADUAdgBoADEASwBRAA==))IEX (New-Object IO.StreamReader(New-Object IO.Compression.GzipStream(,[IO.Compression.CompressionMode]::Decompress))).ReadToEnd();',Cj7g12Zes,user mode service,demand start, malicious\n",
    "2024-08-17T22:23:24Z,7045,60d9b6fa-a407-42ef-94ae-023801db0fb2,doe_admin,%comspec% /b /c start /b /min powershell -nop -w hidden -encodedcommand 'dQA2ADcAbwB1AFEAdAA0AFUARwBmAGMAOAB2AHkAdQBtAEYAdQBiAEcAWgBGAHEAZgBOAEgAegA3AEcAMwBaAHEAdABuAEoAbwBaADUAYwBNAHAARABhAEQAQgBoAHUAegBBADQATQBwAFQAbQBCAFMARABzAGsAZwBiADEAZgBrAFYANwBMAFYAMABZAEwAWQBiAEYAZQA1AE0AZQBQAEoASgBIAGgANwBrADgAdgBUAGEAbgB1AFIAbABQADEASABDAHoAbwA4AHIAVQBRAHcARwBKAFgAUQBMAEsAbwByADcAaAAyAGwAbABYADEAZABuADcARgBwAHgAMwBPADEANQBUAGoARQBZAG8AegAwAHMAZABTAGYANgB1AG8AMQBLAFIAegBXAGEATgBpAGkAUABaADIAbAA3ADcAZwBuAGMAbQBZAGwAQwB4AGYAagBjADcATgBEAFMAawBSAFQAcAA='\"\n",
    "- Service Type: user mode service\n",
    "- Service Start Type: demand start\n",
    "- Account Name: LocalSystem\n",
    "- Data Service Name: Windows11\n",
    "- Timestamp: 2024-08-04T19:18:42Z\n",
    "- ID: 6c7fe4d5-31ed-4fbc-b3bb-xxxxxxxxxxxxM\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user content refers to the input provided by the user during a conversation. It is essentially the message or prompt that the user sends to the model, asking for information, requesting actions, or guiding the conversation. This user input serves as the starting point for the model's response.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the user content for generating log entries\n",
    "user_content_benign = \"Generate a random benign Windows Event ID 7045 log entry.\"\n",
    "user_content_malicious = \"Generate a random malicious Windows Event ID 7045 log entry.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our application now that interfaces with the Llama model on aiml api.\n",
    "Once the api key is beign authenticated, the application pass user input to the model and managing the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    api_key=\"ed4b5e9d497f4d8badf2ed3929bb0c2d\",\n",
    "    base_url=\"https://api.aimlapi.com/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re working with Llama 3.1 to simulate two types of logs: benign logs (safe, expected events) and malicious logs (potentially harmful or suspicious activity). \n",
    "This distinction is important when we’re training models to identify anomalies or threats in systems.\n",
    "\n",
    "We could modify these functions to fine-tune how Llama generates logs, whether we’re looking for specific types of events in the logs or want to test the system’s reaction to more varied types of activity. This is a powerful way to simulate real-world data for AI model training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate benign log\n",
    "def generate_benign_log():\n",
    "    chat_completion_benign = client.chat.completions.create(\n",
    "        #model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n",
    "        model=\"meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": user_content_benign},\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=256,\n",
    "    )\n",
    "    return chat_completion_benign\n",
    "\n",
    "# Generate malicious log\n",
    "def generate_malicious_log():\n",
    "    chat_completion_malicious = client.chat.completions.create(\n",
    "        #model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n",
    "        model=\"meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": user_content_malicious},\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=256,\n",
    "    )\n",
    "    return chat_completion_malicious"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simply creates an empty list called synthetic_logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_logs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're generating synthetic logs by calling the functions generate_benign_log() and generate_malicious_log(), then appending their outputs to the synthetic_logs list. \n",
    "In this case, we generate each log 5 times.\n",
    "\n",
    "We use two loops to generate a total of 10 synthetic logs — 5 benign and 5 malicious. The key functions, generate_benign_log() and generate_malicious_log(), create these logs for us. We then clean up the log content by removing any extra whitespace and store it in the synthetic_logs list. By the end of these loops, we have a collection of logs that we can use for testing or training purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    synthetic_logs.append(generate_benign_log().choices[0].message.content.strip())\n",
    "for i in range(5):\n",
    "    synthetic_logs.append(generate_malicious_log().choices[0].message.content.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will print out each synthetic log stored in synthetic_logs.\n",
    "\n",
    "We review the synthetic logs we’ve generated. By iterating over each log in synthetic_logs, we print the content and add some extra spacing between entries, making it easier to visually inspect each log. This is useful for validating the logs or simply observing how the Llama model generates benign and malicious events.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for log in synthetic_logs:\n",
    "    print(log)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We parse a single log entry, extracting key-value pairs from each line and returning them as a dictionary. \n",
    "\n",
    "This function helps us take a raw log entry, which is essentially a block of text with key-value pairs, and turn it into a structured dictionary. We loop through each line in the log, check for the presence of a key-value pattern (separated by a colon and space), and then store that information in a dictionary. This parsed format is easier to work with, especially when you need to extract specific details from the logs for further analysis or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse a log entry\n",
    "def parse_log_entry(log_entry):\n",
    "    lines = log_entry.split('\\n')\n",
    "    log_data = {}\n",
    "    for line in lines:\n",
    "        if ': ' in line:\n",
    "            key, value = line.split(': ', 1)\n",
    "            log_data[key.strip()] = value.strip()\n",
    "    return log_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build on the parse_log_entry function we discussed earlier.\n",
    "\n",
    "We loop through the synthetic logs that we generated earlier. For each log, we call the parse_log_entry() function to convert the raw log data into a structured dictionary. Then, we store the parsed version of each log in the parsed_logs list. This gives us a clean, structured format for all the logs, which makes it easier to analyze, manipulate, or store in a database for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_logs = []\n",
    "for log in synthetic_logs:\n",
    "    parsed_log = parse_log_entry(log)\n",
    "    parsed_logs.append(parsed_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(parsed_logs)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have learned how to:\n",
    "\n",
    "- Use Llama 3.1 to generate synthetic logs.\n",
    "- Define system and user content to shape model behavior.\n",
    "- Create both benign and malicious logs for simulating real-world data.\n",
    "- Parse unstructured log text into a structured format for easier processing.\n",
    "\n",
    "This syntheic data creation workflow provides a powerful way to simulate data for security, monitoring, or testing systems, and sets a foundation for training anomaly detection AI models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
